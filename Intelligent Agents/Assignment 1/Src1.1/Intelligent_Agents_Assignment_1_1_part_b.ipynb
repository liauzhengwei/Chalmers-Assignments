{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bcBO7k2uCY7D",
        "outputId": "178969d1-103e-4a19-f85b-c2ec10572b94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag.perceptron import PerceptronTagger\n",
        "tagger = PerceptronTagger()"
      ],
      "metadata": {
        "id": "LdT471loFgP6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagger.tag('How does this bear on the question ?'.split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VpPx1bqfF2yC",
        "outputId": "78aa2563-fdfd-4ab4-dc80-1ba4e8ee11a1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('How', 'WRB'),\n",
              " ('does', 'VBZ'),\n",
              " ('this', 'DT'),\n",
              " ('bear', 'NN'),\n",
              " ('on', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('question', 'NN'),\n",
              " ('?', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bear is tagged as a Noun(NN) when it is a Verb(VB)"
      ],
      "metadata": {
        "id": "1la_mCy4GV-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tagger.tag('When you quiet, we can start talking .'.split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hogvZB_MGyYB",
        "outputId": "25c41d51-3813-481c-fa54-12506baf23b2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('When', 'WRB'),\n",
              " ('you', 'PRP'),\n",
              " ('quiet,', 'VBP'),\n",
              " ('we', 'PRP'),\n",
              " ('can', 'MD'),\n",
              " ('start', 'VB'),\n",
              " ('talking', 'VBG'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quiet is correctly tagged as a Verb(VBP)"
      ],
      "metadata": {
        "id": "VrWpr2EEGx9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import files\n",
        "#uploaded = files.upload()"
      ],
      "metadata": {
        "id": "Iw9a5BfMGUDq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the uploaded Brown Corpus text file\n",
        "file_name = 'BrownCorpus.txt'\n",
        "with open(file_name, 'r') as file:\n",
        "    brown_corpus = file.readlines()"
      ],
      "metadata": {
        "id": "h8robrpxHK35"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the conversion file and create a mapping dictionary\n",
        "pos_conversion_dict = {}\n",
        "\n",
        "with open('BrownToUniversalTagMap.txt','r') as f:\n",
        "  for line in f:\n",
        "      original_tag, universal_tag = line.split()\n",
        "      pos_conversion_dict[original_tag] = universal_tag\n",
        "\n",
        "# Check the conversion dictionary\n",
        "print(pos_conversion_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "I1JIZHjdKerl",
        "outputId": "253c8c6b-d295-48c4-e4bc-ce6815d9c899"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"'\": '.', \"''\": '.', '(': '.', '(-HL': '.', ')': '.', ')-HL': '.', '*': 'ADV', '*-HL': 'ADV', '*-NC': 'ADV', '*-TL': 'ADV', ',': '.', ',-HL': '.', ',-NC': '.', ',-TL': '.', '--': '.', '---HL': '.', '.': '.', '.-HL': '.', '.-NC': '.', '.-TL': '.', ':': '.', ':-HL': '.', ':-TL': '.', 'ABL': 'PRT', 'ABN': 'PRT', 'ABN-HL': 'PRT', 'ABN-NC': 'PRT', 'ABN-TL': 'PRT', 'ABX': 'DET', 'AP': 'ADJ', 'AP$': 'PRT', 'AP+AP-NC': 'ADJ', 'AP-HL': 'ADJ', 'AP-NC': 'ADJ', 'AP-TL': 'ADJ', 'AT': 'DET', 'AT-HL': 'DET', 'AT-NC': 'DET', 'AT-TL': 'DET', 'AT-TL-HL': 'DET', 'BE': 'VERB', 'BE-HL': 'VERB', 'BE-TL': 'VERB', 'BED': 'VERB', 'BED*': 'VERB', 'BED-NC': 'VERB', 'BEDZ': 'VERB', 'BEDZ*': 'VERB', 'BEDZ-HL': 'VERB', 'BEDZ-NC': 'VERB', 'BEG': 'VERB', 'BEM': 'VERB', 'BEM*': 'VERB', 'BEM-NC': 'VERB', 'BEN': 'VERB', 'BEN-TL': 'VERB', 'BER': 'VERB', 'BER*': 'VERB', 'BER*-NC': 'VERB', 'BER-HL': 'VERB', 'BER-NC': 'VERB', 'BER-TL': 'VERB', 'BEZ': 'VERB', 'BEZ*': 'VERB', 'BEZ-HL': 'VERB', 'BEZ-NC': 'VERB', 'BEZ-TL': 'VERB', 'CC': 'CONJ', 'CC-HL': 'CONJ', 'CC-NC': 'CONJ', 'CC-TL': 'CONJ', 'CC-TL-HL': 'CONJ', 'CD': 'NUM', 'CD$': 'NOUN', 'CD-HL': 'NUM', 'CD-NC': 'NUM', 'CD-TL': 'NUM', 'CD-TL-HL': 'NUM', 'CS': 'ADP', 'CS-HL': 'ADP', 'CS-NC': 'ADP', 'CS-TL': 'ADP', 'DO': 'VERB', 'DO*': 'VERB', 'DO*-HL': 'VERB', 'DO+PPSS': 'X', 'DO-HL': 'VERB', 'DO-NC': 'VERB', 'DO-TL': 'VERB', 'DOD': 'VERB', 'DOD*': 'VERB', 'DOD*-TL': 'VERB', 'DOD-NC': 'VERB', 'DOZ': 'VERB', 'DOZ*': 'VERB', 'DOZ*-TL': 'VERB', 'DOZ-HL': 'VERB', 'DOZ-TL': 'VERB', 'DT': 'DET', 'DT$': 'DET', 'DT+BEZ': 'PRT', 'DT+BEZ-NC': 'PRT', 'DT+MD': 'PRT', 'DT-HL': 'DET', 'DT-NC': 'DET', 'DT-TL': 'DET', 'DTI': 'DET', 'DTI-HL': 'DET', 'DTI-TL': 'DET', 'DTS': 'DET', 'DTS+BEZ': 'PRT', 'DTS-HL': 'DET', 'DTX': 'DET', 'EX': 'PRT', 'EX+BEZ': 'PRT', 'EX+HVD': 'PRT', 'EX+HVZ': 'PRT', 'EX+MD': 'PRT', 'EX-HL': 'PRT', 'EX-NC': 'PRT', 'FW-*': 'X', 'FW-*-TL': 'X', 'FW-AT': 'X', 'FW-AT+NP': 'X', 'FW-AT+NN': 'X', 'FW-AT+NN-TL': 'X', 'FW-AT+NP-TL': 'X', 'FW-AT-HL': 'X', 'FW-AT-TL': 'X', 'FW-BE': 'X', 'FW-BER': 'X', 'FW-BEZ': 'X', 'FW-CC': 'X', 'FW-CC-TL': 'X', 'FW-CD': 'X', 'FW-CD-TL': 'X', 'FW-CS': 'X', 'FW-DT': 'X', 'FW-DT+BEZ': 'X', 'FW-DTS': 'X', 'FW-HV': 'X', 'FW-IN': 'X', 'FW-IN+AT': 'X', 'FW-IN+AT-T': 'X', 'FW-IN+AT-TL': 'X', 'FW-IN+NN': 'X', 'FW-IN+NN-TL': 'X', 'FW-IN+NP': 'X', 'FW-IN+NP-TL': 'X', 'FW-IN-TL': 'X', 'FW-JJ': 'X', 'FW-JJ-NC': 'X', 'FW-JJ-TL': 'X', 'FW-JJR': 'X', 'FW-JJT': 'X', 'FW-NN': 'X', 'FW-NN$': 'X', 'FW-NN$-TL': 'X', 'FW-NN-NC': 'X', 'FW-NN-TL': 'X', 'FW-NN-TL-NC': 'X', 'FW-NNS': 'X', 'FW-NNS-NC': 'X', 'FW-NNS-TL': 'X', 'FW-NP': 'X', 'FW-NP-TL': 'X', 'FW-NPS': 'X', 'FW-NPS-TL': 'X', 'FW-NR': 'X', 'FW-NR-TL': 'X', 'FW-OD': 'X', 'FW-OD-NC': 'X', 'FW-OD-TL': 'X', 'FW-PN': 'X', 'FW-PP$': 'X', 'FW-PP$-NC': 'X', 'FW-PP$-TL': 'X', 'FW-PPL': 'X', 'FW-PPL+VBZ': 'X', 'FW-PPO': 'X', 'FW-PPO+IN': 'X', 'FW-PPS': 'X', 'FW-PPSS': 'X', 'FW-PPSS+HV': 'X', 'FW-QL': 'X', 'FW-RB': 'X', 'FW-RB+CC': 'X', 'FW-RB-TL': 'X', 'FW-TO+VB': 'X', 'FW-UH': 'X', 'FW-UH-NC': 'X', 'FW-UH-TL': 'X', 'FW-VB': 'X', 'FW-VB-NC': 'X', 'FW-VB-TL': 'X', 'FW-VBD': 'X', 'FW-VBD-TL': 'X', 'FW-VBG': 'X', 'FW-VBG-TL': 'X', 'FW-VBN': 'X', 'FW-VBZ': 'X', 'FW-WDT': 'X', 'FW-WPO': 'X', 'FW-WPS': 'X', 'HV': 'VERB', 'HV*': 'VERB', 'HV+TO': 'VERB', 'HV-HL': 'VERB', 'HV-NC': 'VERB', 'HV-TL': 'VERB', 'HVD': 'VERB', 'HVD*': 'VERB', 'HVD-HL': 'VERB', 'HVG': 'VERB', 'HVG-HL': 'VERB', 'HVN': 'VERB', 'HVZ': 'VERB', 'HVZ*': 'VERB', 'HVZ-NC': 'VERB', 'HVZ-TL': 'VERB', 'IN': 'ADP', 'IN+IN': 'ADP', 'IN+PPO': 'ADP', 'IN-HL': 'ADP', 'IN-NC': 'ADP', 'IN-TL': 'ADP', 'IN-TL-HL': 'ADP', 'JJ': 'ADJ', 'JJ$': 'ADJ', 'JJ$-TL': 'PRT', 'JJ+JJ-NC': 'ADJ', 'JJ-HL': 'ADJ', 'JJ-NC': 'ADJ', 'JJ-TL': 'ADJ', 'JJ-TL-HL': 'ADJ', 'JJ-TL-NC': 'ADJ', 'JJR': 'ADJ', 'JJR+CS': 'ADJ', 'JJR-HL': 'ADJ', 'JJR-NC': 'ADJ', 'JJR-TL': 'ADJ', 'JJS': 'ADJ', 'JJS-HL': 'ADJ', 'JJS-TL': 'ADJ', 'JJT': 'ADJ', 'JJT-HL': 'ADJ', 'JJT-NC': 'ADJ', 'JJT-TL': 'ADJ', 'MD': 'VERB', 'MD*': 'VERB', 'MD*-HL': 'VERB', 'MD+HV': 'VERB', 'MD+PPSS': 'VERB', 'MD+TO': 'VERB', 'MD-HL': 'VERB', 'MD-NC': 'VERB', 'MD-TL': 'VERB', 'NIL': 'X', 'NN': 'NOUN', 'NN$': 'NOUN', 'NN$-HL': 'NOUN', 'NN$-TL': 'NOUN', 'NN+BEZ': 'PRT', 'NN+BEZ-TL': 'PRT', 'NN+HVD': 'PRT', 'NN+HVD-TL': 'PRT', 'NN+HVZ': 'PRT', 'NN+HVZ-TL': 'PRT', 'NN+IN': 'NOUN', 'NN+MD': 'PRT', 'NN+NN-NC': 'NOUN', 'NN-HL': 'NOUN', 'NN-NC': 'NOUN', 'NN-TL': 'NOUN', 'NN-TL-HL': 'NOUN', 'NN-TL-NC': 'NOUN', 'NNS': 'NOUN', 'NNS$': 'NOUN', 'NNS$-HL': 'NOUN', 'NNS$-NC': 'NOUN', 'NNS$-TL': 'NOUN', 'NNS$-TL-HL': 'NOUN', 'NNS+MD': 'PRT', 'NNS-HL': 'NOUN', 'NNS-NC': 'NOUN', 'NNS-TL': 'NOUN', 'NNS-TL-HL': 'NOUN', 'NNS-TL-NC': 'NOUN', 'NP': 'NOUN', 'NP$': 'NOUN', 'NP$-HL': 'NOUN', 'NP$-TL': 'NOUN', 'NP+BEZ': 'PRT', 'NP+BEZ-NC': 'PRT', 'NP+HVZ': 'PRT', 'NP+HVZ-NC': 'PRT', 'NP+MD': 'PRT', 'NP-HL': 'NOUN', 'NP-NC': 'NOUN', 'NP-TL': 'NOUN', 'NP-TL-HL': 'NOUN', 'NPS': 'NOUN', 'NPS$': 'NOUN', 'NPS$-HL': 'NOUN', 'NPS$-TL': 'NOUN', 'NPS-HL': 'NOUN', 'NPS-NC': 'NOUN', 'NPS-TL': 'NOUN', 'NR': 'NOUN', 'NR$': 'NOUN', 'NR$-TL': 'NOUN', 'NR+MD': 'PRT', 'NR-HL': 'NOUN', 'NR-TL': 'NOUN', 'NR-TL-HL': 'NOUN', 'NR-NC': 'NOUN', 'NRS': 'NOUN', 'NRS-TL': 'NOUN', 'OD': 'ADJ', 'OD-HL': 'ADJ', 'OD-NC': 'ADJ', 'OD-TL': 'ADJ', 'PN': 'NOUN', 'PN$': 'NOUN', 'PN+BEZ': 'PRT', 'PN+HVD': 'PRT', 'PN+HVZ': 'PRT', 'PN+MD': 'PRT', 'PN-HL': 'NOUN', 'PN-NC': 'NOUN', 'PN-TL': 'NOUN', 'PP$': 'DET', 'PP$$': 'PRON', 'PP$-HL': 'DET', 'PP$-NC': 'DET', 'PP$-TL': 'DET', 'PPL': 'PRON', 'PPL-HL': 'PRON', 'PPL-NC': 'PRON', 'PPL-TL': 'PRON', 'PPLS': 'PRON', 'PPO': 'PRON', 'PPO-HL': 'PRON', 'PPO-NC': 'PRON', 'PPO-TL': 'PRON', 'PPS': 'PRON', 'PPS+BEZ': 'PRT', 'PPS+BEZ-HL': 'PRT', 'PPS+BEZ-NC': 'PRT', 'PPS+HVD': 'PRT', 'PPS+HVZ': 'PRT', 'PPS+MD': 'PRT', 'PPS-HL': 'PRON', 'PPS-NC': 'PRON', 'PPS-TL': 'PRON', 'PPSS': 'PRON', 'PPSS+BEM': 'PRT', 'PPSS+BER': 'PRT', 'PPSS+BER-N': 'PRT', 'PPSS+BER-NC': 'PRT', 'PPSS+BER-TL': 'PRT', 'PPSS+BEZ': 'PRT', 'PPSS+BEZ*': 'PRT', 'PPSS+HV': 'PRT', 'PPSS+HV-TL': 'PRT', 'PPSS+HVD': 'PRT', 'PPSS+MD': 'PRT', 'PPSS+MD-NC': 'PRT', 'PPSS+VB': 'PRT', 'PPSS-HL': 'PRON', 'PPSS-NC': 'PRON', 'PPSS-TL': 'PRON', 'QL': 'ADV', 'QL-HL': 'ADV', 'QL-NC': 'ADV', 'QL-TL': 'ADV', 'QLP': 'ADV', 'RB': 'ADV', 'RB$': 'PRT', 'RB+BEZ': 'PRT', 'RB+BEZ-HL': 'PRT', 'RB+BEZ-NC': 'PRT', 'RB+CS': 'ADV', 'RB-HL': 'ADV', 'RB-NC': 'ADV', 'RB-TL': 'ADV', 'RBR': 'ADV', 'RBR+CS': 'ADV', 'RBR-NC': 'ADV', 'RBT': 'ADV', 'RN': 'ADV', 'RP': 'PRT', 'RP+IN': 'PRT', 'RP-HL': 'PRT', 'RP-NC': 'PRT', 'RP-TL': 'PRT', 'TO': 'PRT', 'TO+VB': 'PRT', 'TO-HL': 'PRT', 'TO-NC': 'PRT', 'TO-TL': 'PRT', 'UH': 'PRT', 'UH-HL': 'PRT', 'UH-NC': 'PRT', 'UH-TL': 'PRT', 'VB': 'VERB', 'VB+AT': 'VERB', 'VB+IN': 'VERB', 'VB+JJ-NC': 'VERB', 'VB+PPO': 'VERB', 'VB+RP': 'VERB', 'VB+TO': 'VERB', 'VB+VB-NC': 'VERB', 'VB-HL': 'VERB', 'VB-NC': 'VERB', 'VB-TL': 'VERB', 'VBD': 'VERB', 'VBD-HL': 'VERB', 'VBD-NC': 'VERB', 'VBD-TL': 'VERB', 'VBG': 'VERB', 'VBG+TO': 'VERB', 'VBG-HL': 'VERB', 'VBG-NC': 'VERB', 'VBG-TL': 'VERB', 'VBN': 'VERB', 'VBN+TO': 'VERB', 'VBN-HL': 'VERB', 'VBN-NC': 'VERB', 'VBN-TL': 'VERB', 'VBN-TL-HL': 'VERB', 'VBN-TL-NC': 'VERB', 'VBZ': 'VERB', 'VBZ-HL': 'VERB', 'VBZ-NC': 'VERB', 'VBZ-TL': 'VERB', 'WDT': 'DET', 'WDT+BER': 'PRT', 'WDT+BER+PP': 'X', 'WDT+BEZ': 'PRT', 'WDT+BEZ-HL': 'PRT', 'WDT+BEZ-NC': 'PRT', 'WDT+BEZ-TL': 'PRT', 'WDT+DO+PPS': 'X', 'WDT+DOD': 'PRT', 'WDT+HVZ': 'PRT', 'WDT-HL': 'DET', 'WDT-NC': 'DET', 'WP$': 'DET', 'WPO': 'PRON', 'WPO-NC': 'PRON', 'WPO-TL': 'PRON', 'WPS': 'PRON', 'WPS+BEZ': 'PRT', 'WPS+BEZ-NC': 'PRT', 'WPS+BEZ-TL': 'PRT', 'WPS+HVD': 'PRT', 'WPS+HVZ': 'PRT', 'WPS+MD': 'PRT', 'WPS-HL': 'PRON', 'WPS-NC': 'PRON', 'WPS-TL': 'PRON', 'WQL': 'ADV', 'WQL-TL': 'ADV', 'WRB': 'ADV', 'WRB+BER': 'PRT', 'WRB+BEZ': 'PRT', 'WRB+BEZ-TL': 'PRT', 'WRB+DO': 'PRT', 'WRB+DOD': 'PRT', 'WRB+DOD*': 'PRT', 'WRB+DOZ': 'PRT', 'WRB+IN': 'PRT', 'WRB+MD': 'PRT', 'WRB-HL': 'ADV', 'WRB-NC': 'ADV', 'WRB-TL': 'ADV', '``': '.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_sentence(sentence):\n",
        "  processed_sentence = []\n",
        "\n",
        "  for word_tag in sentence:\n",
        "    word,tag = word_tag\n",
        "    if tag in pos_conversion_dict:\n",
        "      tag = pos_conversion_dict[tag]\n",
        "    processed_sentence.append((word.lower(),tag))\n",
        "\n",
        "  return processed_sentence\n",
        "\n",
        "sentences = []\n",
        "for line in brown_corpus:\n",
        "  word_tags = line.strip().split()\n",
        "  sentence = [tuple(word_tag.split('_')) for word_tag in word_tags]\n",
        "  processed_sentence = process_sentence(sentence)\n",
        "  sentences.append(processed_sentence)"
      ],
      "metadata": {
        "id": "061_cQvILgCs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the index for the 80/20 split\n",
        "split_index = int(0.8 * len(sentences))\n",
        "\n",
        "# Training set (first 80% of sentences)\n",
        "training_set = sentences[:split_index]\n",
        "\n",
        "# Testing set (last 20% of sentences)\n",
        "testing_set = sentences[split_index:]\n",
        "\n",
        "print(f\"Training set size: {len(training_set)} sentences\")\n",
        "print(f\"Testing set size: {len(testing_set)} sentences\")\n",
        "\n",
        "print(\"First 5 sentences in training set:\")\n",
        "for sentence in training_set[:5]:\n",
        "    print(sentence)\n",
        "\n",
        "print(\"First 5 sentences in testing set:\")\n",
        "for sentence in testing_set[:5]:\n",
        "    print(sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-KTmTebOnPn",
        "outputId": "7e8ef519-e789-4456-d00e-92c2a3859772"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 45652 sentences\n",
            "Testing set size: 11414 sentences\n",
            "First 5 sentences in training set:\n",
            "[('the', 'DET'), ('fulton', 'NOUN'), ('county', 'NOUN'), ('grand', 'ADJ'), ('jury', 'NOUN'), ('said', 'VERB'), ('friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('no', 'DET'), ('evidence', 'NOUN'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')]\n",
            "[('the', 'DET'), ('jury', 'NOUN'), ('further', 'ADV'), ('said', 'VERB'), ('in', 'ADP'), ('term-end', 'NOUN'), ('presentments', 'NOUN'), ('that', 'ADP'), ('the', 'DET'), ('city', 'NOUN'), ('executive', 'ADJ'), ('committee', 'NOUN'), (',', '.'), ('which', 'DET'), ('had', 'VERB'), ('over-all', 'ADJ'), ('charge', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('election', 'NOUN'), (',', '.'), ('deserves', 'VERB'), ('the', 'DET'), ('praise', 'NOUN'), ('and', 'CONJ'), ('thanks', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('city', 'NOUN'), ('of', 'ADP'), ('atlanta', 'NOUN'), ('for', 'ADP'), ('the', 'DET'), ('manner', 'NOUN'), ('in', 'ADP'), ('which', 'DET'), ('the', 'DET'), ('election', 'NOUN'), ('was', 'VERB'), ('conducted', 'VERB'), ('.', '.')]\n",
            "[('the', 'DET'), ('september-october', 'NOUN'), ('term', 'NOUN'), ('jury', 'NOUN'), ('had', 'VERB'), ('been', 'VERB'), ('charged', 'VERB'), ('by', 'ADP'), ('fulton', 'NOUN'), ('superior', 'ADJ'), ('court', 'NOUN'), ('judge', 'NOUN'), ('durwood', 'NOUN'), ('pye', 'NOUN'), ('to', 'PRT'), ('investigate', 'VERB'), ('reports', 'NOUN'), ('of', 'ADP'), ('possible', 'ADJ'), ('irregularities', 'NOUN'), ('in', 'ADP'), ('the', 'DET'), ('hard-fought', 'ADJ'), ('primary', 'NOUN'), ('which', 'DET'), ('was', 'VERB'), ('won', 'VERB'), ('by', 'ADP'), ('mayor-nominate', 'NOUN'), ('ivan', 'NOUN'), ('allen', 'NOUN'), ('jr.', 'NOUN'), ('.', '.')]\n",
            "[('only', 'ADV'), ('a', 'DET'), ('relative', 'ADJ'), ('handful', 'NOUN'), ('of', 'ADP'), ('such', 'ADJ'), ('reports', 'NOUN'), ('was', 'VERB'), ('received', 'VERB'), (',', '.'), ('the', 'DET'), ('jury', 'NOUN'), ('said', 'VERB'), (',', '.'), ('considering', 'ADP'), ('the', 'DET'), ('widespread', 'ADJ'), ('interest', 'NOUN'), ('in', 'ADP'), ('the', 'DET'), ('election', 'NOUN'), (',', '.'), ('the', 'DET'), ('number', 'NOUN'), ('of', 'ADP'), ('voters', 'NOUN'), ('and', 'CONJ'), ('the', 'DET'), ('size', 'NOUN'), ('of', 'ADP'), ('this', 'DET'), ('city', 'NOUN'), ('.', '.')]\n",
            "[('the', 'DET'), ('jury', 'NOUN'), ('said', 'VERB'), ('it', 'PRON'), ('did', 'VERB'), ('find', 'VERB'), ('that', 'ADP'), ('many', 'ADJ'), ('of', 'ADP'), (\"georgia's\", 'NOUN'), ('registration', 'NOUN'), ('and', 'CONJ'), ('election', 'NOUN'), ('laws', 'NOUN'), ('are', 'VERB'), ('outmoded', 'ADJ'), ('or', 'CONJ'), ('inadequate', 'ADJ'), ('and', 'CONJ'), ('often', 'ADV'), ('ambiguous', 'ADJ'), ('.', '.')]\n",
            "First 5 sentences in testing set:\n",
            "[('they', 'PRON'), ('are', 'VERB'), ('set', 'VERB'), ('forth', 'ADV'), ('in', 'ADP'), ('your', 'DET'), ('own', 'ADJ'), ('newspapers', 'NOUN'), ('.', '.')]\n",
            "[('you', 'PRON'), ('want', 'VERB'), ('from', 'ADP'), ('me', 'PRON'), ('the', 'DET'), ('story', 'NOUN'), (',', '.'), ('but', 'CONJ'), ('a', 'DET'), ('story', 'NOUN'), ('is', 'VERB'), ('about', 'ADP'), ('why', 'ADV'), ('and', 'CONJ'), ('then', 'ADV'), (',', '.'), ('perhaps', 'ADV'), (',', '.'), ('about', 'ADP'), ('how', 'ADV'), ('.', '.')]\n",
            "[('the', 'DET'), ('when', 'ADV'), ('you', 'PRON'), ('know', 'VERB'), (';', '.')]\n",
            "[('yesterday', 'NOUN'), ('morning', 'NOUN'), ('.', '.')]\n",
            "[('so', 'ADP'), ('what', 'DET'), ('i', 'PRON'), ('am', 'VERB'), ('trying', 'VERB'), ('to', 'PRT'), ('tell', 'VERB'), ('you', 'PRON'), ('is', 'VERB'), ('the', 'DET'), ('why', 'ADV'), ('--', '.'), ('that', 'DET'), ('is', 'VERB'), ('my', 'DET'), ('point', 'NOUN'), ('--', '.'), ('and', 'CONJ'), ('that', 'DET'), ('concerns', 'VERB'), ('the', 'DET'), ('spirit', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('matter', 'NOUN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import treebank\n",
        "from nltk.tag.mapping import map_tag\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "# Run the tagger on the test set and convert the Penn tags to Universal tags\n",
        "converted_test_set = []\n",
        "\n",
        "for sentence in testing_set:\n",
        "  # Extract words from the sentence for tagging with the perceptron tagger\n",
        "  words = [word for word, _ in sentence]\n",
        "\n",
        "  # Tag the words using the perceptron tagger\n",
        "  tagged_sentence = tagger.tag(words)\n",
        "\n",
        "  # Map each Penn Treebank POS tag to the Universal POS tag using map_tag\n",
        "  converted_sentence = [(word, map_tag('en-ptb','universal', pos)) for word, pos\n",
        "                        in tagged_sentence]\n",
        "\n",
        "  converted_test_set.append(converted_sentence)\n",
        "\n",
        "for sentence in converted_test_set[:5]:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fz8vqN4AVYE-",
        "outputId": "47204c5c-92fd-412e-8cfd-e557144780e4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('they', 'PRON'), ('are', 'VERB'), ('set', 'VERB'), ('forth', 'NOUN'), ('in', 'ADP'), ('your', 'PRON'), ('own', 'ADJ'), ('newspapers', 'NOUN'), ('.', '.')]\n",
            "[('you', 'PRON'), ('want', 'VERB'), ('from', 'ADP'), ('me', 'PRON'), ('the', 'DET'), ('story', 'NOUN'), (',', '.'), ('but', 'CONJ'), ('a', 'DET'), ('story', 'NOUN'), ('is', 'VERB'), ('about', 'ADP'), ('why', 'ADV'), ('and', 'CONJ'), ('then', 'ADV'), (',', '.'), ('perhaps', 'ADV'), (',', '.'), ('about', 'ADP'), ('how', 'ADV'), ('.', '.')]\n",
            "[('the', 'DET'), ('when', 'ADV'), ('you', 'PRON'), ('know', 'VERB'), (';', '.')]\n",
            "[('yesterday', 'NOUN'), ('morning', 'NOUN'), ('.', '.')]\n",
            "[('so', 'ADP'), ('what', 'PRON'), ('i', 'NOUN'), ('am', 'VERB'), ('trying', 'VERB'), ('to', 'PRT'), ('tell', 'VERB'), ('you', 'PRON'), ('is', 'VERB'), ('the', 'DET'), ('why', 'ADV'), ('--', '.'), ('that', 'DET'), ('is', 'VERB'), ('my', 'PRON'), ('point', 'NOUN'), ('--', '.'), ('and', 'CONJ'), ('that', 'ADP'), ('concerns', 'VERB'), ('the', 'DET'), ('spirit', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('matter', 'NOUN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_positives = 0\n",
        "false_positives = 0\n",
        "false_negatives = 0\n",
        "\n",
        "for i in range(len(testing_set)):\n",
        "  true_sentence = testing_set[i]\n",
        "  predicted_sentence = converted_test_set[i]\n",
        "\n",
        "  #Iterate over words and compare POS tags\n",
        "  for true_word, true_tag in true_sentence:\n",
        "    # Find the corresponding predicted word\n",
        "    predicted_tag = None\n",
        "\n",
        "    for word, pred_tag in predicted_sentence:\n",
        "      if true_word == word:\n",
        "        predicted_tag = pred_tag\n",
        "        break\n",
        "\n",
        "    if predicted_tag == true_tag:\n",
        "      true_positives += 1\n",
        "    elif predicted_tag != true_tag and predicted_tag is not None:\n",
        "      false_positives += 1\n",
        "    elif predicted_tag != true_tag and predicted_tag is None:\n",
        "      false_negatives += 1\n",
        "\n",
        "precision = true_positives / (true_positives + false_positives)\n",
        "recall = true_positives / (true_positives + false_negatives)\n",
        "f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "accuracy = true_positives / (true_positives + false_positives + false_negatives)\n",
        "\n",
        "print(f\"True Positives: {true_positives}\")\n",
        "print(f\"False Positives: {false_positives}\")\n",
        "print(f\"False Negatives: {false_negatives}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1_score}\")\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4iq-cUqXu72",
        "outputId": "d0957139-c9de-4c3d-f3e4-86334f689c06"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True Positives: 154211\n",
            "False Positives: 21830\n",
            "False Negatives: 0\n",
            "Precision: 0.8759947966666856\n",
            "Recall: 1.0\n",
            "F1 Score: 0.9338989620047722\n",
            "Accuracy: 0.8759947966666856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IDzmBVIddZwP"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}